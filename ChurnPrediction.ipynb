{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "CHURN PREDICTION - TELECOM DATA SET\n",
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are going to Load the churners data. The data is available at the following location.\n",
    "https://raw.githubusercontent.com/EricChiang/churn/master/data/churn.csv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the column names the are not the best names, as their is no consistency. \n",
    "The entire data set has been loaded as String and in addition to that the names have spaces too. We need to make sure\n",
    "we can define a proper schema for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schemaString = \"STATE,ACCOUNTLENGTH,AREACODE,PHONE,INTLPLAN,VMAILPLAN,VMAILMESSAGE,DAYMINS,DAYCALLS,DAYCHARGE,EVEMINS,EVECALLS,EVECHARGE,NIGHTMINS,NIGHTCALLS,NIGHTCHARGE,INTLMINS,INTLCALLS,INTLCHARGE,CUSTSERVCALLS,CHURN\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(\",\")]\n",
    "churnSchema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o77.csv.\n: java.net.ConnectException: Call From sparkmaster.demo.com/10.37.101.3 to sparkmaster:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1351)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1300)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1397)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:379)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:379)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:413)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1318)\n\t... 38 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fc754f7878d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchurnDataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchurnSchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hdfs://sparkmaster:8020/user/hdfs/sampledata/churn.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchurnDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/spark/spark-2.0.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/spark/spark-2.0.2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/spark/spark-2.0.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/spark/spark-2.0.2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o77.csv.\n: java.net.ConnectException: Call From sparkmaster.demo.com/10.37.101.3 to sparkmaster:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1351)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1300)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1397)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:379)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:379)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:413)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1318)\n\t... 38 more\n"
     ]
    }
   ],
   "source": [
    "churnDataset = spark.read.option(\"header\",\"true\").schema(churnSchema).csv(\"hdfs://sparkmaster:8020/user/hdfs/sampledata/churn.csv\")\n",
    "cols=churnDataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.createOrReplaceTempView(\"churn_tab\")\n",
    "spark.sql(\"select max(daymins), min(daymins) from churn_tab\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset = churnDataset.withColumn(\"ACCOUNTLENGTH\", churnDataset[\"ACCOUNTLENGTH\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"AREACODE\", churnDataset[\"AREACODE\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"VMAILMESSAGE\", churnDataset[\"VMAILMESSAGE\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"DAYMINS\", churnDataset[\"DAYMINS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"DAYMINS\", churnDataset[\"DAYMINS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"DAYCALLS\", churnDataset[\"DAYCALLS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"DAYCHARGE\", churnDataset[\"DAYCHARGE\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"EVEMINS\", churnDataset[\"EVEMINS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"EVECALLS\", churnDataset[\"EVECALLS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"EVECHARGE\", churnDataset[\"EVECHARGE\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"NIGHTMINS\", churnDataset[\"NIGHTMINS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"NIGHTCALLS\", churnDataset[\"NIGHTCALLS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"NIGHTCHARGE\", churnDataset[\"NIGHTCHARGE\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"INTLMINS\", churnDataset[\"INTLMINS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"INTLCALLS\", churnDataset[\"INTLCALLS\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"INTLCHARGE\", churnDataset[\"INTLCHARGE\"].cast(\"double\"))\n",
    "churnDataset = churnDataset.withColumn(\"CUSTSERVCALLS\", churnDataset[\"CUSTSERVCALLS\"].cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stages = [] # Creating Stages array for our pipeline\n",
    "\n",
    "#Declaring Categorical columns\n",
    "categoricalColumns = [\"PHONE\",\"STATE\", \"INTLPLAN\", \"VMAILPLAN\"]\n",
    "\n",
    "#Looping through the categorical columns for feature transformation\n",
    "for categoricalCol in categoricalColumns:\n",
    "  # Category Indexing with StringIndexer\n",
    "  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "  # Add stages to the stages array. We'll pass these stages to the pipeline.\n",
    "  stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using String indexer to transform Chrun variable\n",
    "label_stringIdx = StringIndexer(inputCol = \"CHURN\", outputCol = \"label\")\n",
    "#Adding the Churn transformation to our pipeline stages\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"ACCOUNTLENGTH\",\"AREACODE\",\"VMAILMESSAGE\",\"DAYMINS\",\"DAYCALLS\",\"DAYCHARGE\",\"EVEMINS\",\"EVECALLS\",\"EVECHARGE\",\"NIGHTMINS\",\"NIGHTCALLS\",\"NIGHTCHARGE\",\"INTLMINS\",\"INTLCALLS\",\"INTLCHARGE\",\"CUSTSERVCALLS\"]\n",
    "#Pick up all the transformed categorical variables\n",
    "categoricalVectorColumns = [*map(lambda c: c + \"classVec\", categoricalColumns)]\n",
    "#Add transformed categorical variables and numberical columns to the assmebler input\n",
    "assemblerInputs = categoricalVectorColumns + numericCols\n",
    "#Use Vector assembler to combine raw numerical features with transformed categorical inputs \n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "#Add the feature assembling part to the pipeline stages\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create the Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(churnDataset)\n",
    "churnDataset = pipelineModel.transform(churnDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(3400,[3056,3349,3382,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399],[1.0,1.0,1.0,128.0,415.0,25.0,265.1,110.0,45.07,197.4,99.0,16.78,244.7,91.0,11.01,10.0,3.0,2.7,1.0])|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churnDataset.select(\"features\").show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "churnDataset = churnDataset.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Randomly split data into training and validation sets\n",
    "(trainingData, testData) = churnDataset.randomSplit([0.7, 0.3], seed = 78799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training size: [2333] === Test Size: [1000]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================================\")\n",
    "print(\"Training size: [\" + str(trainingData.count())+\"] === Test Size: [\"+str(testData.count())+\"]\")\n",
    "print(\"==================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model including timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time is 10.90792441368103s\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5, maxBins=32, numTrees=20)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "\n",
    "# Calculate total time\n",
    "train_time = time.time() - start_time\n",
    "print(\"Training time is \" + str(train_time) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=rfc_2832c50151b2) with 20 trees"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of the model including timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaulation time is 1.014225721359253s\n",
      "Total time for training and evaulation is 11.922150135040283s\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions = rfModel.transform(testData)\n",
    "\n",
    "# Evaluate model. Default metric is areaUnderROC\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate total time\n",
    "eval_time = time.time() - start_time\n",
    "print(\"Evaulation time is \" + str(eval_time) + \"s\")\n",
    "\n",
    "\n",
    "# Print total time for training + evaulation\n",
    "print(\"Total time for training and evaulation is \" + str(train_time + eval_time) + \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve is : 0.8701236787621239s\n"
     ]
    }
   ],
   "source": [
    "print(\"Area under the curve is : \"+str(auc)+\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View model's predictions and probabilities\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|0.0  |0.0       |[0.8617317493137119,0.1382682506862881] |\n",
      "|0.0  |0.0       |[0.8658637362350545,0.13413626376494553]|\n",
      "|0.0  |0.0       |[0.8530759766580674,0.1469240233419326] |\n",
      "|0.0  |0.0       |[0.7845468788124232,0.2154531211875767] |\n",
      "|0.0  |0.0       |[0.8729465847681863,0.12705341523181374]|\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.describe('Churn').show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.describe('Phone','IntlPlan','VMailPlan','VMailMessage','DayMins','DayCalls','DayCharge','Churn').show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.describe('State','EveMins','EveCalls','EveCharge','NightMins','NightCalls','NightCharge').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.describe('AccountLength','AreaCode','IntlMins','IntlCalls','IntlCharge','CustServCalls').show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn = spark.read.option(\"header\",\"true\").csv(\"hdfs://sparkmaster:8020/user/hdfs/sampledata/churn.csv\")\n",
    "\n",
    "churn.createOrReplaceTempView(\"churn_tab\")\n",
    "spark.sql(\"select * from churn_tab limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a DataFrame from a Categorical values to Category vectors so that they can be used by logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (\"AK\", \"Democrats\"),\n",
    "    (\"AS\", \"Republicans\"),\n",
    "    (\"AZ\", \"Democrats\"),\n",
    "    (\"AR\", \"Republicans\"),\n",
    "    (\"CT\", \"GreenParty\"),\n",
    "    (\"DE\", \"Republicans\")\n",
    "], [\"State\", \"winparty\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"winparty\", outputCol=\"winpartyIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"winpartyIndex\", outputCol=\"winpartyVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import mean, min, max, ceil, round\n",
    "churnDataset.select(round((mean('AccountLength')),3)).toDF(\"AccountLength\").show()\n",
    "churnDataset.describe('State','AreaCode','IntlMins','IntlCalls','IntlCharge','CustServCalls','Churn').show()      "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is how you would display a sample chart with Juptyer Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\n",
    "ts = ts.cumsum()\n",
    "ts.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# create an RDD of 100 random numbers\n",
    "x = [random.normalvariate(0,1) for i in range(100)]\n",
    "rdd = sc.parallelize(x)\n",
    "\n",
    "# plot data in RDD - use .collect() to bring data to local\n",
    "num_bins = 50\n",
    "np.array(['1','2','3']).astype(np.float)\n",
    "#n, bins, patches = plt.hist(rdd.collect(), num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "n, bins, patches = plt.hist(np.array(rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to plot the histograms for some of the data types to check their overall distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_bins = 10\n",
    "#n, bins, patches = plt.hist(np.array(churnDataset.select(\"IntlCharge\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"IntlMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"Intl Mins\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"Night Mins\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='blue', alpha=0.5,label=\"Day Mins\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"EveMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='orange', alpha=0.5,label=\"Eve Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"IntlMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"Intl Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"Night Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='blue', alpha=0.5,label=\"Day Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"EveMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='orange', alpha=0.5,label=\"Eve Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.createOrReplaceTempView(\"churn_tab\")\n",
    "churners = spark.sql(\"select * from churn_tab where churn = 'True.'\")\n",
    "churners.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonChurners = spark.sql(\"select * from churn_tab where churn = 'False.'\")\n",
    "nonChurners.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churners.select(\"IntlMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"C Intl Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"IntlMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"NC Intl Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churners.select(\"NightMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"C Night Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"NightMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"NC Night Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churners.select(\"DayMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5,label=\"C Day Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"DayMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5,label=\"NC Day Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churners.select(\"EveMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5,label=\"C Eve Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"EveMins\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5,label=\"NC Eve Mins\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churners.createOrReplaceTempView(\"churner_tab\")\n",
    "spark.sql(\"select Min(IntlCharge), max(IntlCharge) from churner_tab\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churners.select(\"IntlCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"C Intl Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"IntlCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"NC Intl Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churners.select(\"NightCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"C Night Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"NightCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"NC Night Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churners.select(\"DayCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5,label=\"C Day Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"DayCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5,label=\"NC Day Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churners.select(\"EveCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5,label=\"C Eve Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "n, bins, patches = plt.hist(np.array(nonChurners.select(\"EveCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5,label=\"NC Eve Chrg\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import random\n",
    "#num_bins = 10\n",
    "#data=np.vstack([np.array(churnDataset.select(\"DayMins\").rdd.takeSample(True,20,1)).astype(np.float),np.array(churnDataset.select(\"EveMins\").rdd.takeSample(True,20,1)).astype(np.float),np.array(churnDataset.select(\"NightMins\").rdd.takeSample(True,20,1)).astype(np.float)]).T\n",
    "#n, bins, patches = plt.hist(data, num_bins, normed=0, facecolor='green', alpha=0.5, label=[\"Inlt Charge\",\"Night Charge\",\"Day Charge\"])\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "import random\n",
    "num_bins = 10\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"IntlCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"Intl Charge\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='red', alpha=0.5, label=\"Night Charge\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayCharge\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='blue', alpha=0.5,label=\"Day Charge\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_bins = 10\n",
    "\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightCalls\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='Red', alpha=0.5, label=\"Night Calls\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"IntlCalls\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='green', alpha=0.5, label=\"Intl Calls\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayCalls\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='blue', alpha=0.5,label=\"Day Calls\")\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"EveCalls\").rdd.collect()).astype(np.float), num_bins, normed=0, facecolor='Pink', alpha=0.5,label=\"Eve Calls\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.createOrReplaceTempView(\"churn_tab\")\n",
    "churners = spark.sql(\"select * from churn_tab where churn='True.'\")\n",
    "nonChurners = spark.sql(\"select * from churn_tab where churn='False.'\")\n",
    "churners.count()\n",
    "print(\"churners = \"+ str(churners.count()) +\" and non churneres = \"+str(nonChurners.count())+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 50\n",
    "x = np.array(churners.select(\"IntlMins\").rdd.takeSample(False,250,1)).astype(np.float)\n",
    "y = np.array(nonChurners.select(\"IntlMins\").rdd.takeSample(False,250,1)).astype(np.float)\n",
    "colors = ['Red','Green']\n",
    "area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radii\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 50\n",
    "x = np.array(churners.select(\"CustServCalls\").rdd.takeSample(False,350,1)).astype(np.float)\n",
    "y = np.array(nonChurners.select(\"CustServCalls\").rdd.takeSample(False,350,1)).astype(np.float)\n",
    "colors = ['Red','Green']\n",
    "area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radii\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='masifabbasi', api_key='qX37gH9e7nhdEcuV6zSJ')\n",
    "\n",
    "\n",
    "eveMinsChurners = Data([Histogram(x=churners.select('EveMins').rdd.collect())])\n",
    "py.iplot(eveMinsChurners, filename=\"even_minchurners\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Lets look at a scatter plot for Evening Mins for Churners vs. Non churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.tools as tls\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "tls.set_credentials_file(username='masifabbasi', api_key='qX37gH9e7nhdEcuV6zSJ')\n",
    "c = np.array(churners.select(\"CustServCalls\").sample(False,0.9,1).limit(200).rdd.collect()).astype(np.float)\n",
    "nc = np.array(nonChurners.select(\"CustServCalls\").sample(False,0.9,1).limit(200).rdd.collect()).astype(np.float)\n",
    "\n",
    "# N = 1000\n",
    "# random_x = np.random.randn(N)\n",
    "# random_y = np.random.randn(N)\n",
    "# for i,j in zip(c.ravel(),nc.ravel()):\n",
    "Churners = go.Scatter(\n",
    "   y = c.ravel(),\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "      color='red'\n",
    "    )\n",
    ")\n",
    "\n",
    "NonChurners = go.Scatter(\n",
    "   y = nc.ravel(),\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "        color='green'\n",
    "    )\n",
    ")\n",
    "  \n",
    "layout = go.Layout(\n",
    "    title='Customer Service Calls',\n",
    "    xaxis=dict(\n",
    "        title='Customers',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number of Calls to Cust Service',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a trace\n",
    "\n",
    "data = [Churners,NonChurners]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='basic-scatter')\n",
    "\n",
    "# ./\n",
    "# # Plot and embed in ipython notebook!\n",
    "# py.iplot(data, layout=layout, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([7,1,4,8,1,3,2,5])\n",
    "a= np.sort(a)\n",
    "print(\"Array = \"+str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='masifabbasi', api_key='qX37gH9e7nhdEcuV6zSJ')\n",
    "c = np.array(churners.select(\"CustServCalls\").sample(False,0.2,1).limit(400).rdd.collect()).astype(np.float)\n",
    "nc = np.array(nonChurners.select(\"CustServCalls\").sample(False,0.2,1).limit(400).rdd.collect()).astype(np.float)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "N = len(c)\n",
    "random_x=random.sample(range(1, N+2), N)\n",
    "\n",
    "c= np.sort(c)\n",
    "nc = np.sort(nc)\n",
    "random_x = np.sort(random_x)\n",
    "\n",
    "Churners = go.Scatter(\n",
    "   x = random_x,\n",
    "   y = c.ravel(), \n",
    "   name ='Churners',\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "        color='red'\n",
    "    ),\n",
    "  line = dict(\n",
    "            width = 2,\n",
    "            color = 'rgb(0, 0, 0)'\n",
    "        )\n",
    ")\n",
    "\n",
    "NonChurners = go.Scatter(\n",
    "   x = random_x,\n",
    "   y = nc.ravel(),\n",
    "   name = 'Non-Churners',\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "        color='green'\n",
    "    )\n",
    ")\n",
    "  \n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Customer Service Calls',\n",
    "    xaxis=dict(\n",
    "        title='Customers',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number of Calls to Cust Service',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a trace\n",
    "\n",
    "data = [Churners,NonChurners]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='basic-scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#Separating the data into Churn and Non-Churn Data Set\n",
    "churnDataset.createOrReplaceTempView(\"churn_tab\")\n",
    "churners = spark.sql(\"select * from churn_tab where churn='True.'\")\n",
    "nonChurners = spark.sql(\"select * from churn_tab where churn='False.'\")\n",
    "\n",
    "#Getting the Count for churners/Non-Churners\n",
    "churnCnt = churners.count()\n",
    "nonChurnCnt = nonChurners.count()\n",
    "\n",
    "data = [go.Bar(\n",
    "            x=['Churners', 'Non-Churners'],\n",
    "            y=[churnCnt, nonChurnCnt]\n",
    "    )]\n",
    "\n",
    "py.iplot(data, filename='Churn-NonChurn Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='masifabbasi', api_key='qX37gH9e7nhdEcuV6zSJ')\n",
    "c = np.array(churners.select(\"EveMins\").sample(False,0.2,1).limit(200).rdd.collect()).astype(np.float)\n",
    "nc = np.array(non_churners.select(\"EveMins\").sample(False,0.2,1).limit(200).rdd.collect()).astype(np.float)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "# N = 1000\n",
    "# random_x = np.random.randn(N)\n",
    "# random_y = np.random.randn(N)\n",
    "# for i,j in zip(c.ravel(),nc.ravel()):\n",
    "trace = go.Scatter(\n",
    "   x = c.ravel(),\n",
    "   y = nc.ravel(),\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "        color='FFBAD2'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a trace\n",
    "\n",
    "data = [trace]\n",
    "# ./\n",
    "# # Plot and embed in ipython notebook!\n",
    "py.iplot(data, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='masifabbasi', api_key='qX37gH9e7nhdEcuV6zSJ')\n",
    "c = np.array(churners.select(\"EveMins\").sample(False,0.2,1).limit(200).rdd.collect()).astype(np.float)\n",
    "nc = np.array(non_churners.select(\"EveMins\").sample(False,0.2,1).limit(200).rdd.collect()).astype(np.float)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "# N = 1000\n",
    "# random_x = np.random.randn(N)\n",
    "# random_y = np.random.randn(N)\n",
    "# for i,j in zip(c.ravel(),nc.ravel()):\n",
    "Churners = go.Scatter(\n",
    "   x = c.ravel(),\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "        color='red'\n",
    "    )\n",
    ")\n",
    "\n",
    "NonChurners = go.Scatter(\n",
    "   x = nc.ravel(),\n",
    "   mode = 'markers',\n",
    "   marker = dict(\n",
    "        color='blue'\n",
    "    )\n",
    ")\n",
    "  \n",
    "\n",
    "# Create a trace\n",
    "\n",
    "data = [Churners,NonChurners]\n",
    "# ./\n",
    "# # Plot and embed in ipython notebook!\n",
    "py.iplot(data, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1, bins1, patches1 = plt.hist(np.array(churnDataset.select(\"IntlCalls\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='red', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightMins\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='blue', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightCalls\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"NightCharge\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'Phone','IntlPlan','VMailPlan','VMailMessage','DayMins','DayCalls','DayCharge','Churn\n",
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayMins\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayCalls\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(np.array(churnDataset.select(\"DayCharge\").rdd.collect()).astype(np.float), num_bins, normed=1, facecolor='green', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = [(' whitefield', 65299), (' bellandur', 57061), (' kundalahalli', 51769), (' marathahalli', 50639),\n",
    "(' electronic city', 44041), (' sarjapur road junction', 34164), (' indiranagar 2nd stage', 32459),\n",
    "(' malleswaram', 32171), (' yelahanka main road', 28901), (' domlur', 28869)]\n",
    "\n",
    "freequency = []\n",
    "words = []\n",
    "\n",
    "for line in data:\n",
    "    freequency.append(line[1])\n",
    "    words.append(line[0])\n",
    "\n",
    "y_axis = np.arange(1, len(words) + 1, 1)\n",
    "\n",
    "plt.barh(y_axis, freequency, align='center')\n",
    "plt.yticks(y_axis, words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.createOrReplaceTempView(\"churn_tab\")\n",
    "vmailplan = spark.sql(\"select VmailPlan, count(*) as cnt from churn_tab group by VmailPlan \")\n",
    "\n",
    "# plt.barh(y_axis, vmailplan.select(\"cnt\").rdd.collect(), align='center')\n",
    "# plt.yticks(y_axis, vmailplan.select(\"VmailPlan\").rdd.collect())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churnDataset.createOrReplaceTempView(\"churn_tab\")\n",
    "vmailplan = spark.sql(\"select VmailPlan, count(*) as cnt from churn_tab group by VmailPlan \")\n",
    "vmailplan.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_labels= vmailplan['VMAILPLAN'].values\n",
    "fig = vmailplan[['cnt']].plot(kind='bar', facecolor='lightblue')\n",
    "fig.set_xticklabels(x_labels)\n",
    "fig.set_title('Vmail Plans')\n",
    "fig.set_xlabel('Voice Mail Plan ')\n",
    "fig.set_ylabel('Number of People')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# example data\n",
    "mu = 100\n",
    "sigma = 15  # standard deviation of distribution\n",
    "x = mu + sigma * np.random.randn(437)\n",
    "\n",
    "num_bins = 50\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(x, num_bins, normed=1)\n",
    "\n",
    "# add a 'best fit' line\n",
    "y = mlab.normpdf(bins, mu, sigma)\n",
    "ax.plot(bins, y, '--')\n",
    "ax.set_xlabel('Smarts')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.set_title(r'Histogram of IQ: $\\mu=100$, $\\sigma=15$')\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hexbin is an axes method or pyplot function that is essentially\n",
    "a pcolor of a 2-D histogram with hexagonal cells.  It can be\n",
    "much more informative than a scatter plot; in the first subplot\n",
    "below, try substituting 'scatter' for 'hexbin'.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 100000\n",
    "x = np.random.standard_normal(n)\n",
    "y = 2.0 + 3.0 * x + 4.0 * np.random.standard_normal(n)\n",
    "xmin = x.min()\n",
    "xmax = x.max()\n",
    "ymin = y.min()\n",
    "ymax = y.max()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, sharey=True, figsize=(7, 4))\n",
    "fig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\n",
    "ax = axs[0]\n",
    "hb = ax.hexbin(x, y, gridsize=50, cmap='inferno')\n",
    "ax.axis([xmin, xmax, ymin, ymax])\n",
    "ax.set_title(\"Hexagon binning\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('counts')\n",
    "\n",
    "ax = axs[1]\n",
    "hb = ax.hexbin(x, y, gridsize=50, bins='log', cmap='inferno')\n",
    "ax.axis([xmin, xmax, ymin, ymax])\n",
    "ax.set_title(\"With a log color scale\")\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "cb.set_label('log10(N)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple demo of a scatter plot.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 50\n",
    "x = np.random.rand(N)\n",
    "y = np.random.rand(N)\n",
    "colors = np.random.rand(N)\n",
    "area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radii\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(0.0, 2, 0.01)\n",
    "y1 = np.sin(2*np.pi*x)\n",
    "y2 = 1.2*np.sin(4*np.pi*x)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\n",
    "\n",
    "ax1.fill_between(x, 0, y1)\n",
    "ax1.set_ylabel('between y1 and 0')\n",
    "\n",
    "ax2.fill_between(x, y1, 1)\n",
    "ax2.set_ylabel('between y1 and 1')\n",
    "\n",
    "ax3.fill_between(x, y1, y2)\n",
    "ax3.set_ylabel('between y1 and y2')\n",
    "ax3.set_xlabel('x')\n",
    "\n",
    "# now fill between y1 and y2 where a logical condition is met.  Note\n",
    "# this is different than calling\n",
    "#   fill_between(x[where], y1[where],y2[where]\n",
    "# because of edge effects over multiple contiguous regions.\n",
    "fig, (ax, ax1) = plt.subplots(2, 1, sharex=True)\n",
    "ax.plot(x, y1, x, y2, color='black')\n",
    "ax.fill_between(x, y1, y2, where=y2 >= y1, facecolor='green', interpolate=True)\n",
    "ax.fill_between(x, y1, y2, where=y2 <= y1, facecolor='red', interpolate=True)\n",
    "ax.set_title('fill between where')\n",
    "\n",
    "# Test support for masked arrays.\n",
    "y2 = np.ma.masked_greater(y2, 1.0)\n",
    "ax1.plot(x, y1, x, y2, color='black')\n",
    "ax1.fill_between(x, y1, y2, where=y2 >= y1, facecolor='green', interpolate=True)\n",
    "ax1.fill_between(x, y1, y2, where=y2 <= y1, facecolor='red', interpolate=True)\n",
    "ax1.set_title('Now regions with y2>1 are masked')\n",
    "\n",
    "# This example illustrates a problem; because of the data\n",
    "# gridding, there are undesired unfilled triangles at the crossover\n",
    "# points.  A brute-force solution would be to interpolate all\n",
    "# arrays to a very fine grid before plotting.\n",
    "\n",
    "# show how to use transforms to create axes spans where a certain condition is satisfied\n",
    "fig, ax = plt.subplots()\n",
    "y = np.sin(4*np.pi*x)\n",
    "ax.plot(x, y, color='black')\n",
    "\n",
    "# use the data coordinates for the x-axis and the axes coordinates for the y-axis\n",
    "import matplotlib.transforms as mtransforms\n",
    "trans = mtransforms.blended_transform_factory(ax.transData, ax.transAxes)\n",
    "theta = 0.9\n",
    "ax.axhline(theta, color='green', lw=2, alpha=0.5)\n",
    "ax.axhline(-theta, color='red', lw=2, alpha=0.5)\n",
    "ax.fill_between(x, 0, 1, where=y > theta, facecolor='green', alpha=0.5, transform=trans)\n",
    "ax.fill_between(x, 0, 1, where=y < -theta, facecolor='red', alpha=0.5, transform=trans)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Small demonstration of the hlines and vlines plots.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "\n",
    "def f(t):\n",
    "    s1 = np.sin(2 * np.pi * t